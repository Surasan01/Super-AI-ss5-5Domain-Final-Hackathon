{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10a67fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. YOLOv8 is ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Environment Installation\n",
    "\n",
    "# --- ติดตั้ง Libraries ---\n",
    "# ultralytics: Library หลักสำหรับ YOLOv8\n",
    "# opendatasets: สำหรับดาวน์โหลดข้อมูลจาก Kaggle\n",
    "# pycocotools: เครื่องมือสำหรับคำนวณ mAP (YOLO มักจะติดตั้งให้โดยอัตโนมัติ)\n",
    "!pip install ultralytics opendatasets --quiet\n",
    "\n",
    "# --- Import Libraries ที่จำเป็น ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Setup Complete. YOLOv8 is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06036a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \".\\global-wheat-detection\" (use force=True to force download)\n",
      "Data downloaded to: ./global-wheat-detection/\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Acquisition\n",
    "import opendatasets as od\n",
    "\n",
    "# URL ของ Dataset บน Kaggle\n",
    "dataset_url = 'https://www.kaggle.com/c/global-wheat-detection'\n",
    "\n",
    "# ดาวน์โหลดข้อมูล (จะมีการถามหา username และ key จาก kaggle.json)\n",
    "od.download(dataset_url)\n",
    "\n",
    "# --- กำหนด Path หลัก ---\n",
    "DATA_DIR = './global-wheat-detection/'\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_IMG_DIR = os.path.join(DATA_DIR, 'test')\n",
    "TRAIN_CSV_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "\n",
    "print(f\"Data downloaded to: {DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ef702f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848a5164b4e643bbabcf7e0dd3742227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to YOLO format:   0%|          | 0/3373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversion to YOLO format complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Explore and Process Annotations\n",
    "# อ่านไฟล์ CSV\n",
    "df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "# แปลงคอลัมน์ 'bbox' จาก string เป็น list ของตัวเลข\n",
    "df['bbox'] = df['bbox'].apply(lambda x: eval(x))\n",
    "\n",
    "# แยก bbox ออกเป็นแต่ละคอลัมน์\n",
    "df[['x_min', 'y_min', 'width', 'height']] = pd.DataFrame(df['bbox'].tolist(), index=df.index)\n",
    "\n",
    "# ฟังก์ชันสำหรับแปลง Bbox เป็น YOLO format\n",
    "def to_yolo_format(box, img_w, img_h):\n",
    "    x_min, y_min, w, h = box\n",
    "    x_center = (x_min + w / 2) / img_w\n",
    "    y_center = (y_min + h / 2) / img_h\n",
    "    w_norm = w / img_w\n",
    "    h_norm = h / img_h\n",
    "    return f\"0 {x_center} {y_center} {w_norm} {h_norm}\" # class_id คือ 0\n",
    "\n",
    "# สร้างโฟลเดอร์สำหรับเก็บข้อมูลที่แปลงแล้ว\n",
    "os.makedirs('dataset/labels/train', exist_ok=True)\n",
    "os.makedirs('dataset/images/train', exist_ok=True)\n",
    "\n",
    "# วนลูปเพื่อสร้างไฟล์ .txt สำหรับแต่ละภาพ\n",
    "for image_id, group in tqdm(df.groupby('image_id'), desc=\"Converting to YOLO format\"):\n",
    "    # สร้างไฟล์ label\n",
    "    label_path = f\"dataset/labels/train/{image_id}.txt\"\n",
    "    with open(label_path, 'w') as f:\n",
    "        for _, row in group.iterrows():\n",
    "            img_w, img_h = row['width'], row['height']\n",
    "            bbox = [row['x_min'], row['y_min'], row['width'], row['height']]\n",
    "            yolo_str = to_yolo_format(bbox, img_w, img_h)\n",
    "            f.write(yolo_str + '\\n')\n",
    "\n",
    "    # คัดลอกไฟล์ภาพ\n",
    "    shutil.copy(f\"{TRAIN_IMG_DIR}/{image_id}.jpg\", f\"dataset/images/train/{image_id}.jpg\")\n",
    "\n",
    "print(\"\\nConversion to YOLO format complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15356ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbadf8b743f4563b118b781c6b06785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating validation set:   0%|          | 0/674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created train/val split. Train size: 2699, Val size: 674\n",
      "\n",
      "data.yaml file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Train/Val Split and data.yaml\n",
    "\n",
    "# สร้างโฟลเดอร์สำหรับ validation set\n",
    "os.makedirs('dataset/images/val', exist_ok=True)\n",
    "os.makedirs('dataset/labels/val', exist_ok=True)\n",
    "\n",
    "# รายชื่อไฟล์ทั้งหมด\n",
    "all_images = sorted(os.listdir('dataset/images/train'))\n",
    "np.random.shuffle(all_images) # สุ่มลำดับไฟล์\n",
    "\n",
    "# แบ่ง 20% เป็น validation set\n",
    "val_size = int(len(all_images) * 0.2)\n",
    "val_images = all_images[:val_size]\n",
    "train_images = all_images[val_size:]\n",
    "\n",
    "# ย้ายไฟล์ validation\n",
    "for img_name in tqdm(val_images, desc=\"Creating validation set\"):\n",
    "    base_name = os.path.splitext(img_name)[0]\n",
    "    # ย้ายไฟล์ภาพ\n",
    "    shutil.move(f'dataset/images/train/{img_name}', f'dataset/images/val/{img_name}')\n",
    "    # ย้ายไฟล์ label\n",
    "    shutil.move(f'dataset/labels/train/{base_name}.txt', f'dataset/labels/val/{base_name}.txt')\n",
    "\n",
    "print(f\"\\nCreated train/val split. Train size: {len(train_images)}, Val size: {len(val_images)}\")\n",
    "\n",
    "# --- สร้างไฟล์ data.yaml ---\n",
    "data_yaml = {\n",
    "    'train': '../dataset/images/train',\n",
    "    'val': '../dataset/images/val',\n",
    "    'nc': 1, # Number of classes\n",
    "    'names': ['wheat'] # Class names\n",
    "}\n",
    "\n",
    "with open('data.yaml', 'w') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False)\n",
    "\n",
    "print(\"\\ndata.yaml file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a90122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed cache file: dataset\\labels\\train.cache\n",
      "Removed cache file: dataset\\labels\\val.cache\n",
      "Fixing label format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c69acd4c134bf9ae9b15fcbfad49bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Re-creating YOLO labels:   0%|          | 0/3373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label format fixed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Fix Label Format and Train the YOLOv8 Model\n",
    "\n",
    "# First, let's fix the label format issue\n",
    "# Remove the existing cache to force re-processing\n",
    "import glob\n",
    "cache_files = glob.glob('dataset/**/*.cache', recursive=True)\n",
    "for cache_file in cache_files:\n",
    "    if os.path.exists(cache_file):\n",
    "        os.remove(cache_file)\n",
    "        print(f\"Removed cache file: {cache_file}\")\n",
    "\n",
    "# Re-create labels with correct image dimensions\n",
    "print(\"Fixing label format...\")\n",
    "for image_id, group in tqdm(df.groupby('image_id'), desc=\"Re-creating YOLO labels\"):\n",
    "    # Get actual image dimensions\n",
    "    img_path = f\"{TRAIN_IMG_DIR}/{image_id}.jpg\"\n",
    "    if not os.path.exists(img_path):\n",
    "        continue\n",
    "        \n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "        \n",
    "    actual_img_h, actual_img_w = img.shape[:2]\n",
    "    \n",
    "    # Create label file\n",
    "    if image_id in [img.split('.')[0] for img in train_images]:\n",
    "        label_path = f\"dataset/labels/train/{image_id}.txt\"\n",
    "    else:\n",
    "        label_path = f\"dataset/labels/val/{image_id}.txt\"\n",
    "    \n",
    "    with open(label_path, 'w') as f:\n",
    "        for _, row in group.iterrows():\n",
    "            bbox = [row['x_min'], row['y_min'], row['width'], row['height']]\n",
    "            yolo_str = to_yolo_format(bbox, actual_img_w, actual_img_h)\n",
    "            f.write(yolo_str + '\\n')\n",
    "\n",
    "print(\"Label format fixed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b472770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.161  Python-3.12.0 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=wheat_detection_exp14, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\wheat_detection_exp14, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 14.74.4 MB/s, size: 153.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\potij\\OneDrive\\เดสก์ท็อป\\supA\\เก็ง\\CV\\Object Detection\\dataset\\labels\\train... 2699 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2699/2699 [00:07<00:00, 381.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\potij\\OneDrive\\\\supA\\\\CV\\Object Detection\\dataset\\labels\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 148.2152.1 MB/s, size: 183.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\potij\\OneDrive\\เดสก์ท็อป\\supA\\เก็ง\\CV\\Object Detection\\dataset\\labels\\val... 1208 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1208/1208 [00:02<00:00, 426.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\potij\\OneDrive\\\\supA\\\\CV\\Object Detection\\dataset\\labels\\val.cache\n",
      "Plotting labels to runs\\wheat_detection_exp14\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\wheat_detection_exp14\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/169 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 3.16 GiB is free. Of the allocated memory 1.73 GiB is allocated by PyTorch, and 42.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# เริ่มการฝึกสอนโมเดล\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# epochs=25 เป็นค่าเริ่มต้นที่ดีสำหรับการทดลอง (โจทย์จริงอาจใช้ 50-100)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# imgsz=640 คือขนาดภาพที่จะใช้เทรน\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwheat_detection_exp1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel weights and results are saved in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/detect/wheat_detection_exp1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:799\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:406\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m    405\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[1;32m--> 406\u001b[0m     loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:137\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:336\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[1;34m(self, batch, preds)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m--> 336\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(preds, batch)\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:138\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:156\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:179\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 179\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    180\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:319\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    317\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    318\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 3.16 GiB is free. Of the allocated memory 1.73 GiB is allocated by PyTorch, and 42.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# โหลดโมเดล YOLOv8s ที่ pre-trained แล้ว\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# เริ่มการฝึกสอนโมเดล\n",
    "# epochs=25 เป็นค่าเริ่มต้นที่ดีสำหรับการทดลอง (โจทย์จริงอาจใช้ 50-100)\n",
    "# imgsz=640 คือขนาดภาพที่จะใช้เทรน\n",
    "results = model.train(\n",
    "    data='data.yaml',\n",
    "    epochs=25,\n",
    "    imgsz=640,\n",
    "    project='runs',\n",
    "    name='wheat_detection_exp1'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(\"Model weights and results are saved in 'runs/detect/wheat_detection_exp1'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce700848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Inference on a Sample Image\n",
    "\n",
    "# Path ไปยังโมเดลที่ดีที่สุดที่เทรนเสร็จแล้ว\n",
    "BEST_MODEL_PATH = 'runs/detect/wheat_detection_exp1/weights/best.pt'\n",
    "\n",
    "# โหลดโมเดลที่เราเทรนเอง\n",
    "model = YOLO(BEST_MODEL_PATH)\n",
    "\n",
    "# เลือกภาพตัวอย่างจาก validation set\n",
    "sample_image_path = os.path.join('dataset/images/val', np.random.choice(os.listdir('dataset/images/val')))\n",
    "\n",
    "# ทำนายผล\n",
    "results = model(sample_image_path)\n",
    "\n",
    "# วาด Bounding Box ลงบนภาพ\n",
    "annotated_img = results[0].plot()\n",
    "\n",
    "# แสดงผล\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title(f\"Inference result for {os.path.basename(sample_image_path)}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Generate Submission File\n",
    "\n",
    "# โหลดโมเดลที่ดีที่สุด\n",
    "model = YOLO(BEST_MODEL_PATH)\n",
    "\n",
    "# รายชื่อภาพใน test set\n",
    "test_images = os.listdir(TEST_IMG_DIR)\n",
    "\n",
    "submission_data = []\n",
    "\n",
    "# วนลูปทำนายผลใน test set\n",
    "for img_name in tqdm(test_images, desc=\"Generating predictions for submission\"):\n",
    "    img_path = os.path.join(TEST_IMG_DIR, img_name)\n",
    "    image_id = os.path.splitext(img_name)[0]\n",
    "    \n",
    "    # ทำนายผล\n",
    "    results = model(img_path, verbose=False) # verbose=False เพื่อไม่ให้แสดง log เยอะ\n",
    "    \n",
    "    prediction_string = []\n",
    "    # ดึงค่า bbox (xyxy) และ confidence\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy() # [x_min, y_min, x_max, y_max]\n",
    "    confs = results[0].boxes.conf.cpu().numpy()\n",
    "    \n",
    "    for box, conf in zip(boxes, confs):\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        # จัดรูปแบบ string: \"conf x_min y_min w h\"\n",
    "        prediction_string.append(f\"{conf:.4f} {x_min:.2f} {y_min:.2f} {width:.2f} {height:.2f}\")\n",
    "        \n",
    "    # รวมทุก bbox ในภาพเดียวกัน\n",
    "    prediction_string = \" \".join(prediction_string)\n",
    "    submission_data.append({'image_id': image_id, 'PredictionString': prediction_string})\n",
    "\n",
    "# สร้าง DataFrame และบันทึกเป็น CSV\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nsubmission.csv created successfully!\")\n",
    "print(\"Top 5 rows of the submission file:\")\n",
    "display(submission_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
