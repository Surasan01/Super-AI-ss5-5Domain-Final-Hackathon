{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5dbdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: cuda\n"
     ]
    }
   ],
   "source": [
    "# %% 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞ Import Library\n",
    "\n",
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á library ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "!pip install -q transformers[torch] datasets accelerate scikit-learn seqeval\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, Features, Value, ClassLabel, Sequence\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå (GPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8d1bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CORD...\n",
      "\n",
      "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
      "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß:\n",
      "Tokens: ['1', 'KFC', 'Winger', 'HC', '20,000', 'Sub', 'Total', '20,000', 'Dasar', 'Pengenaan', 'Pajak', '20,000', 'P.Rest', '10', '%', '2,000', 'Total', '22,000', 'Cash', '22,000', '1', 'Items,']\n",
      "Tags  : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# %% 2. ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (CORD Dataset)\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CORD...\")\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CORD ‡∏à‡∏≤‡∏Å Hugging Face Hub (‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô train ‡∏°‡∏≤‡∏™‡∏≤‡∏ò‡∏¥‡∏ï)\n",
    "raw_dataset = load_dataset(\"naver-clova-ix/cord-v2\", split=\"train\")\n",
    "\n",
    "# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Label Mapping ---\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Entity ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏à‡∏≤‡∏Å‡πÇ‡∏à‡∏ó‡∏¢‡πå ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Label ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏á\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏° B- ‡πÅ‡∏•‡∏∞ I- prefix ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô IOB2\n",
    "labels = [\"COMPANY\", \"DATE\", \"AMOUNT\"]\n",
    "ner_tags_list = [\"O\"] + [f\"B-{label}\" for label in labels] + [f\"I-{label}\" for label in labels]\n",
    "label2id = {label: i for i, label in enumerate(ner_tags_list)}\n",
    "id2label = {i: label for i, label in enumerate(ner_tags_list)}\n",
    "\n",
    "# Mapping ‡∏à‡∏≤‡∏Å Label ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á CORD ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Label ‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤\n",
    "cord_label_map = {\n",
    "    \"store_name\": \"COMPANY\",\n",
    "    \"payment_date\": \"DATE\",\n",
    "    \"total_price\": \"AMOUNT\"\n",
    "}\n",
    "\n",
    "def process_cord_example(example):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á 1 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å CORD ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö tokens ‡πÅ‡∏•‡∏∞ ner_tags\n",
    "    \"\"\"\n",
    "    # ‡πÇ‡∏´‡∏•‡∏î ground_truth ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô JSON string\n",
    "    ground_truth = json.loads(example[\"ground_truth\"])\n",
    "    \n",
    "    words = []\n",
    "    tags = []\n",
    "    \n",
    "    for item in ground_truth[\"valid_line\"]:\n",
    "        for word_info in item[\"words\"]:\n",
    "            text = word_info[\"text\"]\n",
    "            category = item[\"category\"]\n",
    "            \n",
    "            # ‡πÅ‡∏õ‡∏•‡∏á label ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á cord ‡πÄ‡∏õ‡πá‡∏ô label ‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤\n",
    "            mapped_label = \"O\" # Default ‡∏Ñ‡∏∑‡∏≠ O\n",
    "            if category in cord_label_map:\n",
    "                mapped_label = cord_label_map[category]\n",
    "\n",
    "            # ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ space ‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏≥\n",
    "            sub_words = text.split()\n",
    "            if not sub_words:\n",
    "                continue\n",
    "                \n",
    "            # ‡πÅ‡∏õ‡∏∞‡∏õ‡πâ‡∏≤‡∏¢ B- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å ‡πÅ‡∏•‡∏∞ I- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠\n",
    "            words.append(sub_words[0])\n",
    "            tags.append(f\"B-{mapped_label}\" if mapped_label != \"O\" else \"O\")\n",
    "            \n",
    "            for sub_word in sub_words[1:]:\n",
    "                words.append(sub_word)\n",
    "                tags.append(f\"I-{mapped_label}\" if mapped_label != \"O\" else \"O\")\n",
    "\n",
    "    return {\"tokens\": words, \"ner_tags\": [label2id[tag] for tag in tags]}\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ .map() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "processed_dataset = raw_dataset.map(process_cord_example, remove_columns=raw_dataset.column_names)\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô train ‡πÅ‡∏•‡∏∞ test set\n",
    "dataset_dict = processed_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "print(\"\\n‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "print(\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß:\")\n",
    "example = dataset_dict[\"train\"][1]\n",
    "print(f\"Tokens: {example['tokens']}\")\n",
    "print(f\"Tags  : {[id2label[tag_id] for tag_id in example['ner_tags']]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d61c9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 640\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dda89a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenize ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á Label ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
      "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Tokenize:\n",
      "dict_keys(['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# %% 3. ‡∏Å‡∏≤‡∏£ Tokenize ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á Label\n",
    "\n",
    "# ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å CORD ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• multilingual ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏ò‡∏¥‡∏ï\n",
    "MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Tokenize ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á Label ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Sub-word\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100) # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Special token [CLS], [SEP]\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx]) # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub-word ‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥\n",
    "            else:\n",
    "                label_ids.append(-100) # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub-word ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ .map() ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Tokenize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "tokenized_datasets = dataset_dict.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "print(\"\\nTokenize ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á Label ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "print(\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Tokenize:\")\n",
    "print(tokenized_datasets['train'][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30004ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Model, Arguments, ‡πÅ‡∏•‡∏∞ Metrics ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% 4. ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Fine-tune\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Token Classification ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Label\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(ner_tags_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(DEVICE)\n",
    "\n",
    "# Data Collator ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Padding ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Metrics ‡∏î‡πâ‡∏ß‡∏¢ seqeval\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á ID ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Label ‡πÅ‡∏•‡∏∞‡∏•‡∏ö -100 ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏™‡πà‡πÑ‡∏ß‡πâ\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "import tempfile\n",
    "output_dir = tempfile.mkdtemp(prefix=\"ner_results_\")\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "print(\"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Model, Arguments, ‡πÅ‡∏•‡∏∞ Metrics ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a794cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tempfile\n",
    "\n",
    "# # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏ä‡πâ temporary directory\n",
    "# output_dir = tempfile.mkdtemp(prefix=\"nlp_results_\")\n",
    "\n",
    "# # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ path ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1 ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)\n",
    "# # output_dir = r\"C:\\temp\\nlp_results\"\n",
    "# # os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Training Arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     num_train_epochs=3,                 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å\n",
    "#     per_device_train_batch_size=16,     # ‡∏Ç‡∏ô‡∏≤‡∏î batch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö train\n",
    "#     per_device_eval_batch_size=16,      # ‡∏Ç‡∏ô‡∏≤‡∏î batch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö eval\n",
    "#     warmup_steps=500,                   # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô step ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö warm up learning rate\n",
    "#     weight_decay=0.01,                  # ‡∏Ñ‡πà‡∏≤ weight decay\n",
    "#     logging_steps=100,\n",
    "#     evaluation_strategy=\"epoch\",        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ó‡∏∏‡∏Å‡πÜ 1 epoch\n",
    "#     save_strategy=\"epoch\",              # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏∏‡∏Å‡πÜ 1 epoch\n",
    "#     load_best_model_at_end=True,        # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏´‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à\n",
    "#     metric_for_best_model=\"f1\",         # ‡πÉ‡∏ä‡πâ f1 score ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a31fcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\potij\\AppData\\Local\\Temp\\ipykernel_6388\\3817340510.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ Fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpotijark5\u001b[0m (\u001b[33mpotijark5-no\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\potij\\OneDrive\\‡πÄ‡∏î‡∏™‡∏Å‡πå‡∏ó‡πá‡∏≠‡∏õ\\supA\\‡πÄ‡∏Å‡πá‡∏á\\NLP\\ANLP\\NER_Invoice Extraction\\wandb\\run-20250703_102501-739elaem</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/potijark5-no/huggingface/runs/739elaem' target=\"_blank\">C:\\Users\\potij\\AppData\\Local\\Temp\\ner_results_s3a64nsg</a></strong> to <a href='https://wandb.ai/potijark5-no/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/potijark5-no/huggingface' target=\"_blank\">https://wandb.ai/potijark5-no/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/potijark5-no/huggingface/runs/739elaem' target=\"_blank\">https://wandb.ai/potijark5-no/huggingface/runs/739elaem</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 16:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‡∏Å‡∏≤‡∏£ Fine-tune ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n"
     ]
    }
   ],
   "source": [
    "# %% 5. ‡∏Å‡∏≤‡∏£ Fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ Fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n",
    "# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å\n",
    "trainer.train()\n",
    "print(\"\\n‡∏Å‡∏≤‡∏£ Fine-tune ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76957b4d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd710b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Input: 'Invoice from Tech Solutions Inc. Date: 15/07/2025. Total Amount is $ 4,500.50'\n",
      "\n",
      "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\n",
      "\n",
      "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Dictionary:\n",
      "{'COMPANY': None, 'DATE': None, 'AMOUNT': None}\n"
     ]
    }
   ],
   "source": [
    "# %% 6. ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏• (Inference)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Token Classification ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ aggregation_strategy=\"simple\" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ pipeline ‡∏£‡∏ß‡∏° sub-word ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÉ‡∏ö‡πÅ‡∏à‡πâ‡∏á‡∏´‡∏ô‡∏µ‡πâ\n",
    "sample_text = \"Invoice from Tech Solutions Inc. Date: 15/07/2025. Total Amount is $ 4,500.50\"\n",
    "\n",
    "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
    "results = ner_pipeline(sample_text)\n",
    "\n",
    "print(f\"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Input: '{sample_text}'\")\n",
    "print(\"\\n‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\")\n",
    "for entity in results:\n",
    "    print(f\"  - Entity: {entity['entity_group']}, Value: {entity['word']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "# --- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô Dictionary ---\n",
    "def extract_entities_to_dict(ner_results):\n",
    "    extracted_data = {\"COMPANY\": None, \"DATE\": None, \"AMOUNT\": None}\n",
    "    for entity in ner_results:\n",
    "        entity_type = entity['entity_group']\n",
    "        if entity_type in extracted_data:\n",
    "            extracted_data[entity_type] = entity['word']\n",
    "    return extracted_data\n",
    "\n",
    "final_data = extract_entities_to_dict(results)\n",
    "print(\"\\n‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Dictionary:\")\n",
    "print(final_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca7b597",
   "metadata": {},
   "source": [
    "### Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dcdf461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå submission.csv...\n",
      "\n",
      "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Submission:\n",
      "  invoice_id company_name invoice_date total_amount\n",
      "0    inv_001         None         None         None\n",
      "1    inv_002         None         None         None\n",
      "\n",
      "‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå submission_ner.csv ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n"
     ]
    }
   ],
   "source": [
    "# %% 7. ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Submission (‡∏à‡∏≥‡∏•‡∏≠‡∏á)\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå submission.csv...\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Test DataFrame ‡∏à‡∏≥‡∏•‡∏≠‡∏á\n",
    "test_data = {\n",
    "    \"invoice_id\": [\"inv_001\", \"inv_002\"],\n",
    "    \"invoice_text\": [\n",
    "        \"Receipt from Global Mart LLC, Date: 20/11/2024, FINAL TOTAL: 199.99\",\n",
    "        \"Cyber Systems Ltd. billed you 1,250.00 on 01/01/2025\"\n",
    "    ]\n",
    "}\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
    "predictions = []\n",
    "for text in test_df['invoice_text']:\n",
    "    ner_results = ner_pipeline(text)\n",
    "    extracted_data = extract_entities_to_dict(ner_results)\n",
    "    predictions.append(extracted_data)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "submission_df.columns = [\"company_name\", \"invoice_date\", \"total_amount\"]\n",
    "submission_df.insert(0, 'invoice_id', test_df['invoice_id'])\n",
    "\n",
    "\n",
    "print(\"\\n‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Submission:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV\n",
    "submission_df.to_csv(\"submission_ner.csv\", index=False)\n",
    "\n",
    "print(\"\\n‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå submission_ner.csv ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c1f619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'COMPANY': None, 'DATE': None, 'AMOUNT': None},\n",
       " {'COMPANY': None, 'DATE': None, 'AMOUNT': None}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c7cf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "invoice_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_name",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "invoice_date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "total_amount",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "7dd544e0-8873-4b21-af3e-4165743ab004",
       "rows": [
        [
         "0",
         "inv_001",
         null,
         null,
         null
        ],
        [
         "1",
         "inv_002",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inv_001</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inv_002</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  invoice_id company_name invoice_date total_amount\n",
       "0    inv_001         None         None         None\n",
       "1    inv_002         None         None         None"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
