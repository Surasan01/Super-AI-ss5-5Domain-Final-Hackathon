{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5dbdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "อุปกรณ์ที่ใช้: cuda\n"
     ]
    }
   ],
   "source": [
    "# %% 1. การติดตั้งและ Import Library\n",
    "\n",
    "# ติดตั้ง library ที่จำเป็น\n",
    "!pip install -q transformers[torch] datasets accelerate scikit-learn seqeval\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, Features, Value, ClassLabel, Sequence\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# ตรวจสอบและตั้งค่าอุปกรณ์ (GPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"อุปกรณ์ที่ใช้: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8d1bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กำลังโหลดและประมวลผลข้อมูล CORD...\n",
      "\n",
      "ประมวลผลข้อมูลสำเร็จ\n",
      "ตัวอย่างข้อมูลที่แปลงแล้ว:\n",
      "Tokens: ['1', 'KFC', 'Winger', 'HC', '20,000', 'Sub', 'Total', '20,000', 'Dasar', 'Pengenaan', 'Pajak', '20,000', 'P.Rest', '10', '%', '2,000', 'Total', '22,000', 'Cash', '22,000', '1', 'Items,']\n",
      "Tags  : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# %% 2. การโหลดและแปลงข้อมูลดิบ (CORD Dataset)\n",
    "\n",
    "print(\"กำลังโหลดและประมวลผลข้อมูล CORD...\")\n",
    "# โหลดข้อมูล CORD จาก Hugging Face Hub (ใช้เฉพาะส่วน train มาสาธิต)\n",
    "raw_dataset = load_dataset(\"naver-clova-ix/cord-v2\", split=\"train\")\n",
    "\n",
    "# --- สร้าง Label Mapping ---\n",
    "# กำหนด Entity ที่เราสนใจจากโจทย์ และสร้าง Label ของเราเอง\n",
    "# เพิ่ม B- และ I- prefix ตามมาตรฐาน IOB2\n",
    "labels = [\"COMPANY\", \"DATE\", \"AMOUNT\"]\n",
    "ner_tags_list = [\"O\"] + [f\"B-{label}\" for label in labels] + [f\"I-{label}\" for label in labels]\n",
    "label2id = {label: i for i, label in enumerate(ner_tags_list)}\n",
    "id2label = {i: label for i, label in enumerate(ner_tags_list)}\n",
    "\n",
    "# Mapping จาก Label เดิมของ CORD ไปยัง Label ใหม่ของเรา\n",
    "cord_label_map = {\n",
    "    \"store_name\": \"COMPANY\",\n",
    "    \"payment_date\": \"DATE\",\n",
    "    \"total_price\": \"AMOUNT\"\n",
    "}\n",
    "\n",
    "def process_cord_example(example):\n",
    "    \"\"\"\n",
    "    ฟังก์ชันสำหรับแปลง 1 ตัวอย่างข้อมูลจาก CORD ให้อยู่ในรูปแบบ tokens และ ner_tags\n",
    "    \"\"\"\n",
    "    # โหลด ground_truth ที่เป็น JSON string\n",
    "    ground_truth = json.loads(example[\"ground_truth\"])\n",
    "    \n",
    "    words = []\n",
    "    tags = []\n",
    "    \n",
    "    for item in ground_truth[\"valid_line\"]:\n",
    "        for word_info in item[\"words\"]:\n",
    "            text = word_info[\"text\"]\n",
    "            category = item[\"category\"]\n",
    "            \n",
    "            # แปลง label เดิมของ cord เป็น label ใหม่ของเรา\n",
    "            mapped_label = \"O\" # Default คือ O\n",
    "            if category in cord_label_map:\n",
    "                mapped_label = cord_label_map[category]\n",
    "\n",
    "            # แบ่งคำที่มี space ข้างในออกเป็นหลายๆ คำ\n",
    "            sub_words = text.split()\n",
    "            if not sub_words:\n",
    "                continue\n",
    "                \n",
    "            # แปะป้าย B- ให้คำแรก และ I- ให้คำที่เหลือ\n",
    "            words.append(sub_words[0])\n",
    "            tags.append(f\"B-{mapped_label}\" if mapped_label != \"O\" else \"O\")\n",
    "            \n",
    "            for sub_word in sub_words[1:]:\n",
    "                words.append(sub_word)\n",
    "                tags.append(f\"I-{mapped_label}\" if mapped_label != \"O\" else \"O\")\n",
    "\n",
    "    return {\"tokens\": words, \"ner_tags\": [label2id[tag] for tag in tags]}\n",
    "\n",
    "# ใช้ .map() เพื่อประมวลผลข้อมูลทั้งหมด\n",
    "processed_dataset = raw_dataset.map(process_cord_example, remove_columns=raw_dataset.column_names)\n",
    "\n",
    "# แบ่งข้อมูลเป็น train และ test set\n",
    "dataset_dict = processed_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "print(\"\\nประมวลผลข้อมูลสำเร็จ\")\n",
    "print(\"ตัวอย่างข้อมูลที่แปลงแล้ว:\")\n",
    "example = dataset_dict[\"train\"][1]\n",
    "print(f\"Tokens: {example['tokens']}\")\n",
    "print(f\"Tags  : {[id2label[tag_id] for tag_id in example['ner_tags']]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d61c9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 640\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dda89a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenize และจัดเรียง Label สำเร็จ\n",
      "ตัวอย่างข้อมูลที่ผ่านการ Tokenize:\n",
      "dict_keys(['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# %% 3. การ Tokenize และจัดเรียง Label\n",
    "\n",
    "# เนื่องจาก CORD เป็นภาษาอังกฤษ เราจะใช้โมเดล multilingual เพื่อสาธิต\n",
    "MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    ฟังก์ชันสำหรับ Tokenize และจัดเรียง Label ให้ตรงกับ Sub-word\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100) # สำหรับ Special token [CLS], [SEP]\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx]) # สำหรับ sub-word แรกของคำ\n",
    "            else:\n",
    "                label_ids.append(-100) # สำหรับ sub-word ที่เหลือของคำ\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# ใช้ .map() เพื่อ Tokenize ข้อมูลทั้งหมด\n",
    "tokenized_datasets = dataset_dict.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "print(\"\\nTokenize และจัดเรียง Label สำเร็จ\")\n",
    "print(\"ตัวอย่างข้อมูลที่ผ่านการ Tokenize:\")\n",
    "print(tokenized_datasets['train'][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30004ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ตั้งค่า Model, Arguments, และ Metrics สำเร็จ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% 4. การตั้งค่าสำหรับการ Fine-tune\n",
    "\n",
    "# โหลดโมเดลสำหรับ Token Classification พร้อมระบุจำนวน Label\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(ner_tags_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(DEVICE)\n",
    "\n",
    "# Data Collator จะช่วยจัดการเรื่อง Padding ให้เราอัตโนมัติ\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# กำหนดฟังก์ชันสำหรับคำนวณ Metrics ด้วย seqeval\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # แปลง ID กลับเป็น Label และลบ -100 ที่เราใส่ไว้\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "import tempfile\n",
    "output_dir = tempfile.mkdtemp(prefix=\"ner_results_\")\n",
    "# กำหนด Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "print(\"ตั้งค่า Model, Arguments, และ Metrics สำเร็จ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a794cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tempfile\n",
    "\n",
    "# # วิธีที่ 1: ใช้ temporary directory\n",
    "# output_dir = tempfile.mkdtemp(prefix=\"nlp_results_\")\n",
    "\n",
    "# # วิธีที่ 2: หรือใช้ path ที่เรียบง่าย (ถ้าวิธีที่ 1 ไม่ทำงาน)\n",
    "# # output_dir = r\"C:\\temp\\nlp_results\"\n",
    "# # os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# # กำหนด Training Arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     num_train_epochs=3,                 # จำนวนรอบในการฝึก\n",
    "#     per_device_train_batch_size=16,     # ขนาด batch สำหรับ train\n",
    "#     per_device_eval_batch_size=16,      # ขนาด batch สำหรับ eval\n",
    "#     warmup_steps=500,                   # จำนวน step สำหรับ warm up learning rate\n",
    "#     weight_decay=0.01,                  # ค่า weight decay\n",
    "#     logging_steps=100,\n",
    "#     evaluation_strategy=\"epoch\",        # ประเมินผลทุกๆ 1 epoch\n",
    "#     save_strategy=\"epoch\",              # บันทึกโมเดลทุกๆ 1 epoch\n",
    "#     load_best_model_at_end=True,        # โหลดโมเดลที่ดีที่สุดหลังฝึกเสร็จ\n",
    "#     metric_for_best_model=\"f1\",         # ใช้ f1 score เป็นเกณฑ์เลือกโมเดลที่ดีที่สุด\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a31fcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\potij\\AppData\\Local\\Temp\\ipykernel_6388\\3817340510.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กำลังเริ่มการ Fine-tune โมเดล...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpotijark5\u001b[0m (\u001b[33mpotijark5-no\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\potij\\OneDrive\\เดสก์ท็อป\\supA\\เก็ง\\NLP\\ANLP\\NER_Invoice Extraction\\wandb\\run-20250703_102501-739elaem</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/potijark5-no/huggingface/runs/739elaem' target=\"_blank\">C:\\Users\\potij\\AppData\\Local\\Temp\\ner_results_s3a64nsg</a></strong> to <a href='https://wandb.ai/potijark5-no/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/potijark5-no/huggingface' target=\"_blank\">https://wandb.ai/potijark5-no/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/potijark5-no/huggingface/runs/739elaem' target=\"_blank\">https://wandb.ai/potijark5-no/huggingface/runs/739elaem</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 16:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\potij\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "การ Fine-tune เสร็จสิ้น\n"
     ]
    }
   ],
   "source": [
    "# %% 5. การ Fine-tune โมเดล\n",
    "\n",
    "# สร้าง Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"กำลังเริ่มการ Fine-tune โมเดล...\")\n",
    "# เริ่มฝึก\n",
    "trainer.train()\n",
    "print(\"\\nการ Fine-tune เสร็จสิ้น\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76957b4d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd710b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ข้อความ Input: 'Invoice from Tech Solutions Inc. Date: 15/07/2025. Total Amount is $ 4,500.50'\n",
      "\n",
      "ผลลัพธ์การสกัดข้อมูล:\n",
      "\n",
      "ผลลัพธ์ในรูปแบบ Dictionary:\n",
      "{'COMPANY': None, 'DATE': None, 'AMOUNT': None}\n"
     ]
    }
   ],
   "source": [
    "# %% 6. การทดสอบทำนายผล (Inference)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# สร้าง pipeline สำหรับ Token Classification จากโมเดลที่เราเพิ่งฝึกเสร็จ\n",
    "# ตั้งค่า aggregation_strategy=\"simple\" เพื่อให้ pipeline รวม sub-word ให้เราอัตโนมัติ\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# ข้อความตัวอย่างที่จำลองมาจากใบแจ้งหนี้\n",
    "sample_text = \"Invoice from Tech Solutions Inc. Date: 15/07/2025. Total Amount is $ 4,500.50\"\n",
    "\n",
    "# ทำนายผล\n",
    "results = ner_pipeline(sample_text)\n",
    "\n",
    "print(f\"ข้อความ Input: '{sample_text}'\")\n",
    "print(\"\\nผลลัพธ์การสกัดข้อมูล:\")\n",
    "for entity in results:\n",
    "    print(f\"  - Entity: {entity['entity_group']}, Value: {entity['word']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "# --- เขียนฟังก์ชันง่ายๆ เพื่อแปลงผลลัพธ์เป็น Dictionary ---\n",
    "def extract_entities_to_dict(ner_results):\n",
    "    extracted_data = {\"COMPANY\": None, \"DATE\": None, \"AMOUNT\": None}\n",
    "    for entity in ner_results:\n",
    "        entity_type = entity['entity_group']\n",
    "        if entity_type in extracted_data:\n",
    "            extracted_data[entity_type] = entity['word']\n",
    "    return extracted_data\n",
    "\n",
    "final_data = extract_entities_to_dict(results)\n",
    "print(\"\\nผลลัพธ์ในรูปแบบ Dictionary:\")\n",
    "print(final_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca7b597",
   "metadata": {},
   "source": [
    "### Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dcdf461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กำลังจำลองการสร้างไฟล์ submission.csv...\n",
      "\n",
      "ตัวอย่างข้อมูลในไฟล์ Submission:\n",
      "  invoice_id company_name invoice_date total_amount\n",
      "0    inv_001         None         None         None\n",
      "1    inv_002         None         None         None\n",
      "\n",
      "สร้างไฟล์ submission_ner.csv สำเร็จ!\n"
     ]
    }
   ],
   "source": [
    "# %% 7. การสร้างไฟล์ Submission (จำลอง)\n",
    "\n",
    "print(\"กำลังจำลองการสร้างไฟล์ submission.csv...\")\n",
    "\n",
    "# สร้าง Test DataFrame จำลอง\n",
    "test_data = {\n",
    "    \"invoice_id\": [\"inv_001\", \"inv_002\"],\n",
    "    \"invoice_text\": [\n",
    "        \"Receipt from Global Mart LLC, Date: 20/11/2024, FINAL TOTAL: 199.99\",\n",
    "        \"Cyber Systems Ltd. billed you 1,250.00 on 01/01/2025\"\n",
    "    ]\n",
    "}\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# วนลูปเพื่อทำนายผลแต่ละรายการ\n",
    "predictions = []\n",
    "for text in test_df['invoice_text']:\n",
    "    ner_results = ner_pipeline(text)\n",
    "    extracted_data = extract_entities_to_dict(ner_results)\n",
    "    predictions.append(extracted_data)\n",
    "\n",
    "# สร้าง DataFrame จากผลลัพธ์\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "submission_df.columns = [\"company_name\", \"invoice_date\", \"total_amount\"]\n",
    "submission_df.insert(0, 'invoice_id', test_df['invoice_id'])\n",
    "\n",
    "\n",
    "print(\"\\nตัวอย่างข้อมูลในไฟล์ Submission:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# บันทึกเป็นไฟล์ CSV\n",
    "submission_df.to_csv(\"submission_ner.csv\", index=False)\n",
    "\n",
    "print(\"\\nสร้างไฟล์ submission_ner.csv สำเร็จ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c1f619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'COMPANY': None, 'DATE': None, 'AMOUNT': None},\n",
       " {'COMPANY': None, 'DATE': None, 'AMOUNT': None}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c7cf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "invoice_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_name",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "invoice_date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "total_amount",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "7dd544e0-8873-4b21-af3e-4165743ab004",
       "rows": [
        [
         "0",
         "inv_001",
         null,
         null,
         null
        ],
        [
         "1",
         "inv_002",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inv_001</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inv_002</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  invoice_id company_name invoice_date total_amount\n",
       "0    inv_001         None         None         None\n",
       "1    inv_002         None         None         None"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
